{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')  # Add src directory to path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n",
      "c:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "from deepxdeAbeta import deepxde as dde\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.integrate import odeint\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default float type to float64\n",
      "Setting the default backend to \"pytorch\". You can change it in the ~/.deepxde/config.json file or export the DDE_BACKEND environment variable. Valid options are: tensorflow.compat.v1, tensorflow, pytorch, jax, paddle (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "dde.config.set_default_float(\"float64\")\n",
    "dde.backend.set_default_backend(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 10\n",
    "num_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_npz(filepath, out_path, x_cols, y_cols):\n",
    "    df = pd.read_csv(filepath)\n",
    "    t = []\n",
    "    y = []\n",
    "    for _, v in df.iterrows():\n",
    "        t.append(v[x_cols].to_numpy())\n",
    "        y.append(v[y_cols].to_numpy())\n",
    "    np.savez(out_path, t=t, y=y)\n",
    "\n",
    "\n",
    "def load_training_data(data_path):\n",
    "    tr_data = np.load(data_path)\n",
    "    return tr_data['t'], tr_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = dde.geometry.TimeDomain(0, max_time)\n",
    "\n",
    "\n",
    "# Helper function that is used to check whether a point is an initial point or not. This is only used by DeepXDE\n",
    "def boundary(_, on_initial):\n",
    "    return on_initial\n",
    "\n",
    "\n",
    "num_hidden_layers = 3\n",
    "hidden_layer_size = 128\n",
    "output_layer = 3\n",
    "\n",
    "layers = [1] + [hidden_layer_size] * num_hidden_layers + [output_layer]\n",
    "\n",
    "activation = ['tanh']*num_hidden_layers + ['tanh']\n",
    "\n",
    "iterations = 35_000\n",
    "optimizer = \"adam\"\n",
    "learning_rate = 1e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 2 #TODO\n",
    "model_dir = '../results/models/model' + str(model_num)\n",
    "datafile = '../data/model' + str(model_num) + '.csv'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "# from binning_trials.binned_models import *\n",
    "a1, a2, a3, a4 = [dde.Variable(np.float64(0)), dde.Variable(np.float64(0)), dde.Variable(np.float64(0)), dde.Variable(np.float64(0))]\n",
    "n, m = np.float64(1.7), np.float64(4/1.7)\n",
    "def equations(x, y):\n",
    "    # print(y)\n",
    "    B1, B1_7, B4 = abs(y[:, 0:1]), abs(y[:, 1:2]), abs(y[:, 2:3])\n",
    "    # tf.print(x, output_stream=sys.stderr, summarize=10)\n",
    "\n",
    "    B1t = dde.grad.jacobian(y, x, i=0)\n",
    "    B1_7t = dde.grad.jacobian(y, x, i=1)\n",
    "    B4t = dde.grad.jacobian(y, x, i=2)\n",
    "    # print(B1_35)\n",
    "\n",
    "    # calculates residuals by subtracting right side from left side of each equation.\n",
    "    # Left side is calculated with respect to time, right side is calculated with the other species.\n",
    "    # Finding the correct constants to make residuals = 0; essentially residual = loss to be minimized.\n",
    "    r_b1 = B1t - (a1 * n * B1_7 - a2 * n * B1**n)\n",
    "    r_b1_7 = B1_7t - (a2 * B1**n - a1 * B1_7 + a3 * m * B4 - a4 * m * B1_7**m)\n",
    "    r_b4 = B4t - (a4 * B1_7**m - a3 * B4)\n",
    "\n",
    "\n",
    "    # print(r_b1_35)\n",
    "\n",
    "    # res = torch.cat([r_b1, r_b1p, r_bn, r_bnp, r_bm, r_bmp])\n",
    "    # diff = torch.cat([B1t, B1pt, Bnt, Bnpt, Bmt, Bmpt])\n",
    "    # pred = torch.cat([B1tp, B1ptp, Bntp, Bnptp, Bmtp, Bmptp])\n",
    "    # res2 = torch.where(pred < 0, res*100, res)\n",
    "    # # print(res[torch.ge(res, diff)])\n",
    "    #\n",
    "    # return torch.chunk(res, 6)\n",
    "\n",
    "    return [r_b1, r_b1_7, r_b4]\n",
    "initial_conditions = [1.39E-06,4.04E-06,2.39E-05]\n",
    "y_cols = ['1uM', '1.7uM', '4uM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "'compile' took 2.465323 s\n",
      "\n",
      "Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.\n",
      "Training model...\n",
      "\n",
      "Step      Train loss                                                                Test loss                                                                 Test metric\n",
      "0         [8.74e-04, 4.20e-03, 7.94e-03, 1.93e-12, 1.63e-11, 4.14e-01, 1.37e+00]    [8.74e-04, 4.20e-03, 7.94e-03, 1.93e-12, 1.63e-11, 4.14e-01, 1.37e+00]    []  \n",
      "350       [5.22e-04, 3.21e-03, 3.04e-04, 5.47e-07, 1.29e-04, 3.31e-03, 3.63e-03]    [5.22e-04, 3.21e-03, 3.04e-04, 5.47e-07, 1.29e-04, 3.31e-03, 3.63e-03]    []  \n",
      "700       [7.62e-04, 3.37e-03, 4.40e-04, 1.57e-06, 1.26e-04, 2.47e-03, 3.31e-03]    [7.62e-04, 3.37e-03, 4.40e-04, 1.57e-06, 1.26e-04, 2.47e-03, 3.31e-03]    []  \n",
      "1050      [8.73e-04, 3.43e-03, 5.33e-04, 1.96e-06, 1.25e-04, 2.06e-03, 3.14e-03]    [8.73e-04, 3.43e-03, 5.33e-04, 1.96e-06, 1.25e-04, 2.06e-03, 3.14e-03]    []  \n",
      "1400      [1.03e-03, 3.35e-03, 7.00e-04, 2.45e-07, 1.35e-04, 2.07e-03, 3.08e-03]    [1.03e-03, 3.35e-03, 7.00e-04, 2.45e-07, 1.35e-04, 2.07e-03, 3.08e-03]    []  \n",
      "1750      [1.02e-03, 3.42e-03, 7.28e-04, 1.62e-06, 1.29e-04, 1.55e-03, 2.86e-03]    [1.02e-03, 3.42e-03, 7.28e-04, 1.62e-06, 1.29e-04, 1.55e-03, 2.86e-03]    []  \n",
      "2100      [1.07e-03, 3.46e-03, 7.54e-04, 1.65e-06, 1.33e-04, 1.43e-03, 2.80e-03]    [1.07e-03, 3.46e-03, 7.54e-04, 1.65e-06, 1.33e-04, 1.43e-03, 2.80e-03]    []  \n",
      "2450      [1.09e-03, 3.49e-03, 7.64e-04, 1.75e-06, 1.34e-04, 1.37e-03, 2.78e-03]    [1.09e-03, 3.49e-03, 7.64e-04, 1.75e-06, 1.34e-04, 1.37e-03, 2.78e-03]    []  \n",
      "2800      [1.11e-03, 3.51e-03, 7.70e-04, 1.82e-06, 1.34e-04, 1.32e-03, 2.76e-03]    [1.11e-03, 3.51e-03, 7.70e-04, 1.82e-06, 1.34e-04, 1.32e-03, 2.76e-03]    []  \n",
      "3150      [1.12e-03, 3.52e-03, 7.66e-04, 2.11e-06, 1.36e-04, 1.29e-03, 2.76e-03]    [1.12e-03, 3.52e-03, 7.66e-04, 2.11e-06, 1.36e-04, 1.29e-03, 2.76e-03]    []  \n",
      "3500      [1.13e-03, 3.53e-03, 7.62e-04, 1.46e-06, 1.28e-04, 1.27e-03, 2.75e-03]    [1.13e-03, 3.53e-03, 7.62e-04, 1.46e-06, 1.28e-04, 1.27e-03, 2.75e-03]    []  \n",
      "3850      [1.13e-03, 3.47e-03, 8.34e-04, 2.84e-06, 1.22e-04, 1.27e-03, 2.77e-03]    [1.13e-03, 3.47e-03, 8.34e-04, 2.84e-06, 1.22e-04, 1.27e-03, 2.77e-03]    []  \n",
      "4200      [1.10e-03, 3.47e-03, 8.43e-04, 1.21e-05, 1.44e-04, 1.46e-03, 2.76e-03]    [1.10e-03, 3.47e-03, 8.43e-04, 1.21e-05, 1.44e-04, 1.46e-03, 2.76e-03]    []  \n",
      "4550      [1.18e-03, 3.52e-03, 7.55e-04, 4.37e-06, 1.15e-04, 1.22e-03, 2.80e-03]    [1.18e-03, 3.52e-03, 7.55e-04, 4.37e-06, 1.15e-04, 1.22e-03, 2.80e-03]    []  \n",
      "4900      [1.15e-03, 3.56e-03, 7.30e-04, 1.33e-06, 1.34e-04, 1.21e-03, 2.75e-03]    [1.15e-03, 3.56e-03, 7.30e-04, 1.33e-06, 1.34e-04, 1.21e-03, 2.75e-03]    []  \n",
      "5250      [1.16e-03, 3.50e-03, 7.75e-04, 1.76e-06, 1.23e-04, 1.19e-03, 2.77e-03]    [1.16e-03, 3.50e-03, 7.75e-04, 1.76e-06, 1.23e-04, 1.19e-03, 2.77e-03]    []  \n",
      "5600      [1.14e-03, 3.54e-03, 7.23e-04, 4.04e-06, 1.38e-04, 1.22e-03, 2.76e-03]    [1.14e-03, 3.54e-03, 7.23e-04, 4.04e-06, 1.38e-04, 1.22e-03, 2.76e-03]    []  \n",
      "5950      [1.15e-03, 3.45e-03, 8.94e-04, 1.07e-05, 8.71e-05, 1.25e-03, 2.83e-03]    [1.15e-03, 3.45e-03, 8.94e-04, 1.07e-05, 8.71e-05, 1.25e-03, 2.83e-03]    []  \n",
      "6300      [1.24e-03, 3.57e-03, 7.14e-04, 1.22e-06, 2.77e-04, 1.23e-03, 3.77e-03]    [1.24e-03, 3.57e-03, 7.14e-04, 1.22e-06, 2.77e-04, 1.23e-03, 3.77e-03]    []  \n",
      "6650      [1.15e-03, 3.48e-03, 7.62e-04, 1.89e-06, 1.33e-04, 1.17e-03, 2.76e-03]    [1.15e-03, 3.48e-03, 7.62e-04, 1.89e-06, 1.33e-04, 1.17e-03, 2.76e-03]    []  \n",
      "7000      [1.13e-03, 3.44e-03, 7.82e-04, 4.71e-06, 1.21e-04, 1.24e-03, 2.79e-03]    [1.13e-03, 3.44e-03, 7.82e-04, 4.71e-06, 1.21e-04, 1.24e-03, 2.79e-03]    []  \n",
      "7350      [1.15e-03, 3.48e-03, 7.47e-04, 1.67e-06, 1.39e-04, 1.16e-03, 2.75e-03]    [1.15e-03, 3.48e-03, 7.47e-04, 1.67e-06, 1.39e-04, 1.16e-03, 2.75e-03]    []  \n",
      "7700      [1.16e-03, 3.46e-03, 7.63e-04, 2.07e-06, 1.35e-04, 1.16e-03, 2.75e-03]    [1.16e-03, 3.46e-03, 7.63e-04, 2.07e-06, 1.35e-04, 1.16e-03, 2.75e-03]    []  \n",
      "8050      [1.17e-03, 3.43e-03, 7.95e-04, 2.37e-06, 1.11e-04, 1.15e-03, 2.78e-03]    [1.17e-03, 3.43e-03, 7.95e-04, 2.37e-06, 1.11e-04, 1.15e-03, 2.78e-03]    []  \n",
      "8400      [1.15e-03, 3.57e-03, 7.27e-04, 1.88e-06, 1.95e-04, 1.17e-03, 2.69e-03]    [1.15e-03, 3.57e-03, 7.27e-04, 1.88e-06, 1.95e-04, 1.17e-03, 2.69e-03]    []  \n",
      "8750      [1.06e-03, 3.67e-03, 7.37e-04, 7.06e-07, 2.75e-04, 1.45e-03, 2.77e-03]    [1.06e-03, 3.67e-03, 7.37e-04, 7.06e-07, 2.75e-04, 1.45e-03, 2.77e-03]    []  \n",
      "9100      [1.18e-03, 3.42e-03, 8.03e-04, 6.81e-05, 6.40e-05, 1.30e-03, 2.95e-03]    [1.18e-03, 3.42e-03, 8.03e-04, 6.81e-05, 6.40e-05, 1.30e-03, 2.95e-03]    []  \n",
      "9450      [1.15e-03, 3.41e-03, 7.98e-04, 3.99e-06, 1.56e-04, 1.16e-03, 2.75e-03]    [1.15e-03, 3.41e-03, 7.98e-04, 3.99e-06, 1.56e-04, 1.16e-03, 2.75e-03]    []  \n",
      "9800      [1.16e-03, 3.47e-03, 7.81e-04, 2.89e-08, 1.51e-04, 1.16e-03, 2.71e-03]    [1.16e-03, 3.47e-03, 7.81e-04, 2.89e-08, 1.51e-04, 1.16e-03, 2.71e-03]    []  \n",
      "10150     [1.19e-03, 3.42e-03, 7.63e-04, 5.54e-06, 1.80e-04, 1.23e-03, 3.02e-03]    [1.19e-03, 3.42e-03, 7.63e-04, 5.54e-06, 1.80e-04, 1.23e-03, 3.02e-03]    []  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m model = dde.Model(data, network)\n\u001b[32m     29\u001b[39m model.compile(optimizer, lr=learning_rate, external_trainable_variables=[a1, a2, a3, a4])\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss_history, train_state = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m=\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m99\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisregard_previous_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m dde.saveplot(loss_history, train_state, issave=\u001b[38;5;28;01mTrue\u001b[39;00m, isplot=\u001b[38;5;28;01mFalse\u001b[39;00m, output_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m dde.utils.external.plot_loss_history(loss_history,\n\u001b[32m     41\u001b[39m                                      fname=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/loss_history\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     42\u001b[39m                                      )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\utils\\internal.py:22\u001b[39m, in \u001b[36mtiming.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     ts = timeit.default_timer()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     te = timeit.default_timer()\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.rank == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\model.py:651\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[39m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m iterations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    650\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo iterations for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.opt_name))\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks.on_train_end()\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.rank == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\model.py:669\u001b[39m, in \u001b[36mModel._train_sgd\u001b[39m\u001b[34m(self, iterations, display_every)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks.on_batch_begin()\n\u001b[32m    666\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.set_data_train(\n\u001b[32m    667\u001b[39m     *\u001b[38;5;28mself\u001b[39m.data.train_next_batch(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    668\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.epoch += \u001b[32m1\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\model.py:563\u001b[39m, in \u001b[36mModel._train_step\u001b[39m\u001b[34m(self, inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_step(inputs, targets, auxiliary_vars)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state = \u001b[38;5;28mself\u001b[39m.train_step(\n\u001b[32m    567\u001b[39m         \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state, inputs, targets\n\u001b[32m    568\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\model.py:359\u001b[39m, in \u001b[36mModel._compile_pytorch.<locals>.train_step\u001b[39m\u001b[34m(inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    356\u001b[39m     total_loss.backward()\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    228\u001b[39m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog_backup\\notebooks\\../src\\deepxdeAbeta\\deepxde\\model.py:356\u001b[39m, in \u001b[36mModel._compile_pytorch.<locals>.train_step.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    354\u001b[39m total_loss = torch.sum(losses)\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m.opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "variable = dde.callbacks.VariableValue(\n",
    "    [a1, a2, a3, a4],\n",
    "    period=(iterations//100 if iterations > 99 else 1),\n",
    "    filename=model_dir+'/variables.dat')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "to_npz(datafile, '../data/model' + str(model_num) + '.npz', x_cols=['t'], y_cols=y_cols)\n",
    "ob_t, ob_y = load_training_data('../data/model' + str(model_num) + '.npz')\n",
    "# print(ob_y1.values, ob_y2.values, ob_y3.values)\n",
    "ic1 = dde.icbc.IC(geom, lambda X: initial_conditions[0], boundary, component=0)\n",
    "ic2 = dde.icbc.IC(geom, lambda X: initial_conditions[1], boundary, component=1)\n",
    "\n",
    "ob_y1 = dde.icbc.PointSetBC(ob_t, ob_y[:, 0:1], component=0)\n",
    "ob_y2 = dde.icbc.PointSetBC(ob_t, ob_y[:, 1:2], component=1)\n",
    "\n",
    "data = dde.data.PDE(\n",
    "    geom,\n",
    "    equations,\n",
    "    [ic1, ic2, ob_y1, ob_y2],\n",
    "    num_domain=200, #TODO\n",
    "    num_boundary=1, #TODO\n",
    "    anchors=ob_t\n",
    ")\n",
    "\n",
    "network = dde.nn.FNN(layers, activation, 'Glorot uniform')\n",
    "model = dde.Model(data, network)\n",
    "\n",
    "model.compile(optimizer, lr=learning_rate, external_trainable_variables=[a1, a2, a3, a4])\n",
    "\n",
    "loss_history, train_state = model.train(\n",
    "    epochs=iterations, callbacks=[variable], display_every=\n",
    "    (iterations // 100 if iterations > 99 else 1)\n",
    "    ,\n",
    "    disregard_previous_best=True\n",
    ")\n",
    "\n",
    "\n",
    "dde.saveplot(loss_history, train_state, issave=True, isplot=False, output_dir=f'{model_dir}')\n",
    "dde.utils.external.plot_loss_history(loss_history,\n",
    "                                     fname=f'{model_dir}/loss_history'\n",
    "                                     )\n",
    "dde.utils.external.plot_best_state(train_state,\n",
    "                                   fname=f'{model_dir}/train_state'\n",
    "                                   )\n",
    "\n",
    "pred = model.predict(ob_t, operator=equations)\n",
    "model.save(f'{model_dir}/nonneg_constr')\n",
    "\n",
    "with open(f'{model_dir}/info.dat', 'x') as f:\n",
    "\n",
    "    lines = [\n",
    "        f'training time: {time.time() - start}\\n',\n",
    "        f'residual: {np.mean(np.absolute(pred))}\\n'\n",
    "        f'best model at: {train_state.best_step}\\n'\n",
    "        f'train loss: {train_state.best_loss_train}\\n'\n",
    "    ]\n",
    "\n",
    "    f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
