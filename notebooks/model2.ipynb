{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "from deepxdeAbeta import deepxde as dde\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.integrate import odeint\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default float type to float64\n",
      "Setting the default backend to \"pytorch\". You can change it in the ~/.deepxde/config.json file or export the DDE_BACKEND environment variable. Valid options are: tensorflow.compat.v1, tensorflow, pytorch, jax, paddle (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "dde.config.set_default_float(\"float64\")\n",
    "dde.backend.set_default_backend(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 10\n",
    "num_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_npz(filepath, out_path, x_cols, y_cols):\n",
    "    df = pd.read_csv(filepath)\n",
    "    t = []\n",
    "    y = []\n",
    "    for _, v in df.iterrows():\n",
    "        t.append(v[x_cols].to_numpy())\n",
    "        y.append(v[y_cols].to_numpy())\n",
    "    np.savez(out_path, t=t, y=y)\n",
    "\n",
    "\n",
    "def load_training_data(data_path):\n",
    "    tr_data = np.load(data_path)\n",
    "    return tr_data['t'], tr_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = dde.geometry.TimeDomain(0, max_time)\n",
    "\n",
    "\n",
    "# Helper function that is used to check whether a point is an initial point or not. This is only used by DeepXDE\n",
    "def boundary(_, on_initial):\n",
    "    return on_initial\n",
    "\n",
    "\n",
    "num_hidden_layers = 3\n",
    "hidden_layer_size = 128\n",
    "output_layer = 3\n",
    "\n",
    "layers = [1] + [hidden_layer_size] * num_hidden_layers + [output_layer]\n",
    "\n",
    "activation = ['tanh']*num_hidden_layers + ['tanh']\n",
    "\n",
    "iterations = 35_000\n",
    "optimizer = \"adam\"\n",
    "learning_rate = 1e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 2 #TODO\n",
    "model_dir = 'binning_trials/models/model' + str(model_num)\n",
    "datafile = 'binning_trials/model' + str(model_num) + '.csv'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "# from binning_trials.binned_models import *\n",
    "a1, a2, a3, a4 = [dde.Variable(np.float64(0)), dde.Variable(np.float64(0)), dde.Variable(np.float64(0)), dde.Variable(np.float64(0))]\n",
    "n, m = np.float64(1.7), np.float64(4/1.7)\n",
    "def equations(x, y):\n",
    "    # print(y)\n",
    "    B1, B1_7, B4 = abs(y[:, 0:1]), abs(y[:, 1:2]), abs(y[:, 2:3])\n",
    "    # tf.print(x, output_stream=sys.stderr, summarize=10)\n",
    "\n",
    "    B1t = dde.grad.jacobian(y, x, i=0)\n",
    "    B1_7t = dde.grad.jacobian(y, x, i=1)\n",
    "    B4t = dde.grad.jacobian(y, x, i=2)\n",
    "    # print(B1_35)\n",
    "\n",
    "    # calculates residuals by subtracting right side from left side of each equation.\n",
    "    # Left side is calculated with respect to time, right side is calculated with the other species.\n",
    "    # Finding the correct constants to make residuals = 0; essentially residual = loss to be minimized.\n",
    "    r_b1 = B1t - (a1 * n * B1_7 - a2 * n * B1**n)\n",
    "    r_b1_7 = B1_7t - (a2 * B1**n - a1 * B1_7 + a3 * m * B4 - a4 * m * B1_7**m)\n",
    "    r_b4 = B4t - (a4 * B1_7**m - a3 * B4)\n",
    "\n",
    "\n",
    "    # print(r_b1_35)\n",
    "\n",
    "    # res = torch.cat([r_b1, r_b1p, r_bn, r_bnp, r_bm, r_bmp])\n",
    "    # diff = torch.cat([B1t, B1pt, Bnt, Bnpt, Bmt, Bmpt])\n",
    "    # pred = torch.cat([B1tp, B1ptp, Bntp, Bnptp, Bmtp, Bmptp])\n",
    "    # res2 = torch.where(pred < 0, res*100, res)\n",
    "    # # print(res[torch.ge(res, diff)])\n",
    "    #\n",
    "    # return torch.chunk(res, 6)\n",
    "\n",
    "    return [r_b1, r_b1_7, r_b4]\n",
    "initial_conditions = [1.39E-06,4.04E-06,2.39E-05]\n",
    "y_cols = ['1uM', '1.7uM', '4uM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "'compile' took 0.000997 s\n",
      "\n",
      "Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.\n",
      "Training model...\n",
      "\n",
      "0         [5.48e-04, 3.03e-04, 7.97e-04, 1.93e-12, 1.63e-11, 3.75e-01, 4.65e-01]    [5.48e-04, 3.03e-04, 7.97e-04, 1.93e-12, 1.63e-11, 3.75e-01, 4.65e-01]    []  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m model = dde.Model(data, network)\n\u001b[32m     29\u001b[39m model.compile(optimizer, lr=learning_rate, external_trainable_variables=[a1, a2, a3, a4])\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m loss_history, train_state = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m=\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m99\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisregard_previous_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m dde.saveplot(loss_history, train_state, issave=\u001b[38;5;28;01mTrue\u001b[39;00m, isplot=\u001b[38;5;28;01mFalse\u001b[39;00m, output_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m dde.utils.external.plot_loss_history(loss_history,\n\u001b[32m     41\u001b[39m                                      fname=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/loss_history\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     42\u001b[39m                                      )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\utils\\internal.py:22\u001b[39m, in \u001b[36mtiming.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     ts = timeit.default_timer()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     te = timeit.default_timer()\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.rank == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\model.py:651\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[39m\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m iterations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    650\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo iterations for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.opt_name))\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks.on_train_end()\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.rank == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\model.py:669\u001b[39m, in \u001b[36mModel._train_sgd\u001b[39m\u001b[34m(self, iterations, display_every)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks.on_batch_begin()\n\u001b[32m    666\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.set_data_train(\n\u001b[32m    667\u001b[39m     *\u001b[38;5;28mself\u001b[39m.data.train_next_batch(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    668\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.epoch += \u001b[32m1\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\model.py:563\u001b[39m, in \u001b[36mModel._train_step\u001b[39m\u001b[34m(self, inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_step(inputs, targets, auxiliary_vars)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state = \u001b[38;5;28mself\u001b[39m.train_step(\n\u001b[32m    567\u001b[39m         \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state, inputs, targets\n\u001b[32m    568\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\model.py:359\u001b[39m, in \u001b[36mModel._compile_pytorch.<locals>.train_step\u001b[39m\u001b[34m(inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    356\u001b[39m     total_loss.backward()\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:146\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    149\u001b[39m     params_with_grad = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\Documents\\k0de\\git\\abettaPINNog\\deepxdeAbeta\\deepxde\\model.py:356\u001b[39m, in \u001b[36mModel._compile_pytorch.<locals>.train_step.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    354\u001b[39m total_loss = torch.sum(losses)\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m.opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "variable = dde.callbacks.VariableValue(\n",
    "    [a1, a2, a3, a4],\n",
    "    period=(iterations//100 if iterations > 99 else 1),\n",
    "    filename=model_dir+'/variables.dat')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "to_npz(datafile, 'model' + str(model_num) + '.npz', x_cols=['t'], y_cols=y_cols)\n",
    "ob_t, ob_y = load_training_data('model' + str(model_num) + '.npz')\n",
    "# print(ob_y1.values, ob_y2.values, ob_y3.values)\n",
    "ic1 = dde.icbc.IC(geom, lambda X: initial_conditions[0], boundary, component=0)\n",
    "ic2 = dde.icbc.IC(geom, lambda X: initial_conditions[1], boundary, component=1)\n",
    "\n",
    "ob_y1 = dde.icbc.PointSetBC(ob_t, ob_y[:, 0:1], component=0)\n",
    "ob_y2 = dde.icbc.PointSetBC(ob_t, ob_y[:, 1:2], component=1)\n",
    "\n",
    "data = dde.data.PDE(\n",
    "    geom,\n",
    "    equations,\n",
    "    [ic1, ic2, ob_y1, ob_y2],\n",
    "    num_domain=200, #TODO\n",
    "    num_boundary=1, #TODO\n",
    "    anchors=ob_t\n",
    ")\n",
    "\n",
    "network = dde.nn.FNN(layers, activation, 'Glorot uniform')\n",
    "model = dde.Model(data, network)\n",
    "\n",
    "model.compile(optimizer, lr=learning_rate, external_trainable_variables=[a1, a2, a3, a4])\n",
    "\n",
    "loss_history, train_state = model.train(\n",
    "    epochs=iterations, callbacks=[variable], display_every=\n",
    "    (iterations // 100 if iterations > 99 else 1)\n",
    "    ,\n",
    "    disregard_previous_best=True\n",
    ")\n",
    "\n",
    "\n",
    "dde.saveplot(loss_history, train_state, issave=True, isplot=False, output_dir=f'{model_dir}')\n",
    "dde.utils.external.plot_loss_history(loss_history,\n",
    "                                     fname=f'{model_dir}/loss_history'\n",
    "                                     )\n",
    "dde.utils.external.plot_best_state(train_state,\n",
    "                                   fname=f'{model_dir}/train_state'\n",
    "                                   )\n",
    "\n",
    "pred = model.predict(ob_t, operator=equations)\n",
    "model.save(f'{model_dir}/nonneg_constr')\n",
    "\n",
    "with open(f'{model_dir}/info.dat', 'x') as f:\n",
    "\n",
    "    lines = [\n",
    "        f'training time: {time.time() - start}\\n',\n",
    "        f'residual: {np.mean(np.absolute(pred))}\\n'\n",
    "        f'best model at: {train_state.best_step}\\n'\n",
    "        f'train loss: {train_state.best_loss_train}\\n'\n",
    "    ]\n",
    "\n",
    "    f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
